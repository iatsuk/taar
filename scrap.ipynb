{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipes scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ru_RU.UTF-8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from lxml import html, etree\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, 'ru_RU.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseScrapper:\n",
    "\n",
    "    def __init__(self, user_agent: str = UserAgent().random, crawl_delay: int = 0):\n",
    "        self._crawl_delay = crawl_delay\n",
    "        self._last_item_urls: set = set()\n",
    "        self._data_to_return: list = []\n",
    "        self._session = requests.Session()\n",
    "        self._session.headers.update({'User-Agent': user_agent,})\n",
    "\n",
    "    def __iter__(self) -> iter:\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self._data_to_return) == 0:\n",
    "            data = self._crawl_data()\n",
    "            item_urls = set([item['url'] for item in data])\n",
    "            if self._last_item_urls != item_urls:\n",
    "                self._last_item_urls = item_urls\n",
    "                self._data_to_return = data\n",
    "                return self.__next__()\n",
    "            else:\n",
    "                raise StopIteration\n",
    "        else:\n",
    "            return self._data_to_return.pop()\n",
    "\n",
    "    def _crawl_data(self) -> set:\n",
    "        url = self._next_url()\n",
    "        page = self._get_page(url)\n",
    "        data = self._extract_data(page)\n",
    "        return data\n",
    "\n",
    "    def _get_page(self, url) -> html.HtmlElement:\n",
    "        time.sleep(self._crawl_delay)\n",
    "        content = self._session.get(url).text\n",
    "        if (self._session.status_code != requests.codes.ok):\n",
    "            raise Exception('Response code', self._session.status_code, 'for url', url)\n",
    "        parsed = html.fromstring(content)\n",
    "        return parsed\n",
    "\n",
    "    def _extract_data(self, parsed: html.HtmlElement) -> list:\n",
    "        data = []\n",
    "        items = self._get_items(parsed)\n",
    "        for item in items:\n",
    "            try:\n",
    "                data.append(self._parse_item(item))\n",
    "            except Exception as e:\n",
    "                print('item:', etree.tostring(item))\n",
    "                raise e\n",
    "        return data\n",
    "\n",
    "    def _next_url(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _get_items(self, page: html.HtmlElement) -> list:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _parse_item(self, item: html.HtmlElement) -> dict:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RussianFoodScrapper(BaseScrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _next_url(self) -> str:\n",
    "        host = 'https://www.russianfood.com'\n",
    "        start_page = 1\n",
    "        end_page = 220\n",
    "        if not self._cur_page:\n",
    "            self._cur_page = start_page\n",
    "        elif self._cur_page <= 220:\n",
    "            self._cur_page += 1\n",
    "        return f'{host}/?page={self._cur_page}'\n",
    "\n",
    "    def _get_items(self, page: html.HtmlElement) -> list:\n",
    "        return page.xpath('//div[@class=\"annonce annonce_orange\"]')\n",
    "\n",
    "    def _parse_item(self, item: html.HtmlElement) -> dict:\n",
    "        return {\n",
    "            'raw': etree.tostring(article),\n",
    "            'url': 'https://www.russianfood.com' + item.xpath('.//table[@class=\"blog_content_table\"]//noindex/a[@class=\"detail\"]/@href')[0], \n",
    "            'name': unicodedata.normalize('NFKD', article.xpath('.//span[@class=\"newsitem__title-inner\"]/text()')[0]),\n",
    "            'ingr': datetime.strptime(article.xpath('.//div[@class=\"newsitem__params\"]/span[contains(@class,\"js-ago\")]/@datetime')[0], '%Y-%m-%dT%H:%M:%S%z'),\n",
    "            'recipe': '',\n",
    "            'rating': unicodedata.normalize('NFKD', article.xpath('.//div[@class=\"newsitem__params\"]/span[@class=\"newsitem__param\"]/text()')[0]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mongodb docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a30840108e9e6d7f28141e005ef0024c6b8d01b52b7cd03c7bc88e7ab564c2b2\n"
     ]
    }
   ],
   "source": [
    "!docker run -d -p 27017:27017 --name mongodb mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start mongo db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker start mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run spiders and fill database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo = MongoClient('localhost', 27017)\n",
    "db = mongo['food_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_via_scrapper(recipes, collection):\n",
    "    for recipe in recipes:\n",
    "        if collection.update_one({'_id': recipe['url']}, {'$set': recipe}, upsert=True).matched_count != 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dump from mongodb container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec -it mongodb mongodump --out=/backup/ --db=food_db --collection=recipes \n",
    "!docker exec -it mongodb tar czf dump.mongo.tgz /backup\n",
    "!docker cp mongodb:/dump.mongo.tgz dump.mongo.tgz\n",
    "!docker exec -it mongodb rm -rf /backup /dump.mongo.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put dump into mongodb container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp dump.mongo.tgz mongodb:/dump.mongo.tgz\n",
    "!docker exec -it mongodb tar xzf dump.mongo.tgz\n",
    "!docker exec -it mongodb mongorestore /backup\n",
    "!docker exec -it mongodb rm -rf /backup /dump.mongo.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shutdown mongo db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop mongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ingr, collection, limit):\n",
    "    for recipe in collection.find({'ingr': ingr}, {'name': 1, 'ingr': 1, 'url': 1}):\n",
    "        print(recipe['name'])\n",
    "        print('url:', recipe['url'])\n",
    "        pprint(recipe['ingr'])\n",
    "        print()\n",
    "        limit -= 1\n",
    "        if limit <= 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data('фарш', food_db.recipes, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
